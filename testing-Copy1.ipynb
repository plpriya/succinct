{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ppriya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ppriya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# %load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_doc(id, dataset):\n",
    "    #print(dataset[id])\n",
    "    file = open(dataset[id][0], 'r', encoding='cp1250')\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "    #print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data) #needed again as we need to stem the words\n",
    "    data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
    "    data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genDataSet(start, end):\n",
    "    dataset = []\n",
    "\n",
    "    c = False\n",
    "\n",
    "    folders = [x[0] for x in os.walk('C:\\\\Users\\\\ppriya\\\\OneDrive - NetApp Inc\\\\Downloads\\\\stories\\\\stories')]\n",
    "\n",
    "    for i in folders:\n",
    "        file = open(i+\"\\\\index.html\", 'r')\n",
    "        text = file.read().strip()\n",
    "        file.close()\n",
    "\n",
    "        file_name = re.findall('><A HREF=\"(.*)\">', text)\n",
    "        file_title = re.findall('<BR><TD> (.*)\\n', text)\n",
    "\n",
    "        if c == False:\n",
    "            file_name = file_name[2:]\n",
    "            c = True\n",
    "\n",
    "        #print(len(file_name), len(file_title))\n",
    "\n",
    "        for j in range(len(file_name)):\n",
    "            dataset.append((str(i) +\"/\"+ str(file_name[j]), file_title[j]))\n",
    "    return dataset[start:end]\n",
    "    \n",
    "def extractData(dataset) :\n",
    "    processed_text = []\n",
    "    processed_title = []\n",
    "\n",
    "    for i in dataset:\n",
    "        file = open(i[0], 'r', encoding=\"utf8\", errors='ignore')\n",
    "        text = file.read().strip()\n",
    "        file.close()\n",
    "\n",
    "        processed_text.append(word_tokenize(str(preprocess(text))))\n",
    "        processed_title.append(word_tokenize(str(preprocess(i[1]))))\n",
    "    return processed_text, processed_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genDocFreqForAllDocs(processed_text, processed_title, start, end) :\n",
    "    DF = {}\n",
    "\n",
    "    for i in range(len(processed_text)):\n",
    "        tokens = processed_text[i ]\n",
    "        for w in tokens:\n",
    "            try:\n",
    "                DF[w].add(i + start)\n",
    "            except:\n",
    "                DF[w] = {i + start}\n",
    "\n",
    "        tokens = processed_title[i]\n",
    "        for w in tokens:\n",
    "            try:\n",
    "                DF[w].add(i + start)\n",
    "            except:\n",
    "                DF[w] = {i + start}\n",
    "    for i in DF:\n",
    "        DF[i] = len(DF[i])\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_freq(DF, word):\n",
    "    c = 0\n",
    "    try:\n",
    "        c = DF[word]\n",
    "    except:\n",
    "        pass\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocumentMetadata(processed_text, processed_title, start, end) :\n",
    "    DMText = {}\n",
    "    DMTitle = {}\n",
    "\n",
    "    for i in range(len(processed_text)):\n",
    "        tokens = processed_text[i]\n",
    "        counter = Counter(tokens)\n",
    "        for token in np.unique(tokens):\n",
    "            try:\n",
    "                DMText[i + start].add(token, counter[token])\n",
    "            except:\n",
    "                DMText[i + start] = {token, counter[token]}\n",
    "\n",
    "\n",
    "    for i in range(len(processed_text)):\n",
    "        tokens = processed_title[i]\n",
    "        counter = Counter(tokens)\n",
    "        for token in np.unique(tokens):\n",
    "            try:\n",
    "                DMTitle[i+ start].add(token, counter[token])\n",
    "            except:\n",
    "                DMTitle[i + start] = {token, counter[token]}\n",
    "\n",
    "    return DMText, DMTitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def termFreq(DM, term, doc_id) :\n",
    "    tf = 0\n",
    "    try:\n",
    "        tf = DM[doc_id][term]\n",
    "    except:\n",
    "        pass\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(PL, DMTitle, DMText, term, doc_id):\n",
    "    alpha = 0.3\n",
    "    tf = (1-alpha) * termFreq(DMTitle, term, doc_id) + alpha * termFreq(DMText, term, doc_id)\n",
    "    df = docFreq(PL, token)\n",
    "    idf = np.log((N+1)/(df+1))\n",
    "    return td*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildIndex(start, end) :\n",
    "    dataset = genDataSet(start, end)\n",
    "    processed_text, processed_title = extractData(dataset)\n",
    "    DF = genDocFreqForAllDocs(processed_text, processed_title, start, end)\n",
    "    DMText, DMTitle = getDocumentMetadata(processed_text, processed_title, start, end)\n",
    "    return DF, DMText, DMTitle\n",
    "    #tfidf = calcDFIDF(DF, processed_text, processed_title)\n",
    "    \n",
    "def plMerge(pl1, pl2):\n",
    "    #Merge dictionaries and keep values of common keys in list\n",
    "    pl = {}\n",
    "    for key, value in pl1.items():\n",
    "        if key in pl2:\n",
    "            pl[key] = pl1[key] + pl2[key]\n",
    "        else :\n",
    "            pl[key] = pl1[key]\n",
    "    for key, value in pl2.items():\n",
    "        if key not in pl1:\n",
    "            pl[key] = pl2[key]\n",
    "    return pl\n",
    "\n",
    "def mergeIndices(pl1, pl2, dmtext1, dmtext2, dmtitle1, dmtitle2): #pl - posting list, dm - document metadata\n",
    "    dmtext = {}\n",
    "    dmtitle = {}\n",
    "    pl = {}\n",
    "    dmtext = {**dmtext1, **dmtext2}\n",
    "    dmtitle = {**dmtitle1, **dmtitle2}\n",
    "    pl = plMerge(pl1, pl2)\n",
    "    return pl, dmtext, dmtitle\n",
    "\n",
    "def queryfunc(k, query):\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "\n",
    "    print(\"My Query Function\")\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\")\n",
    "    print(tokens)\n",
    "\n",
    "    PL, DMText, DMTitle = buildIndex(title)\n",
    "    query_weights = {}\n",
    "    for w in tokens:\n",
    "        for d in DF[w]:\n",
    "            try:\n",
    "                query_weights[d] += tfidf(PL, DMText, DMTitle, w, d)\n",
    "            except:\n",
    "                query_weights[d] = tdidf(PL, DMText, DMTitle, w, d)\n",
    "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"\")\n",
    "    \n",
    "    l = []\n",
    "    \n",
    "    for i in query_weights[:k]:\n",
    "        l.append(i[0])\n",
    "    \n",
    "    #print(l)\n",
    "    return l\n",
    "import time\n",
    "\n",
    "def testIndexes():\n",
    "    \n",
    "    start = time.process_time()\n",
    "    DF1, DMText1, DMTitle1 = buildIndex(0, 100)\n",
    "    print(time.process_time() - start)\n",
    "    start = time.process_time()\n",
    "    DF2, DMText2, DMTitle2 = buildIndex(100, 200)\n",
    "    print(time.process_time() - start)\n",
    "    DF, DMText, DMTitle = buildIndex(0, 200)\n",
    "    start = time.process_time()\n",
    "    mergedDF, mergedDMText, mergedDMTitle = mergeIndices(DF1, DF2, DMText1, DMText2, DMTitle1, DMTitle2)\n",
    "    print(time.process_time() - start)\n",
    "    print(mergedDMText == DMText)\n",
    "    print (mergedDMTitle == DMTitle)\n",
    "    print(DF == mergedDF)\n",
    "    print(len(DMText1.keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.234375\n",
      "27.578125\n",
      "0.0\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "testIndexes()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
